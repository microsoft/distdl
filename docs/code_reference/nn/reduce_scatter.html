<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>ReduceScatter Layer &#8212; DistDL 0.5.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/pydoctheme.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script src="../../_static/sidebar.js"></script>
    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Repartition Layer" href="repartition.html" />
    <link rel="prev" title="Pooling Layers" href="pooling.html" />
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <link rel="shortcut icon" type="image/png" href="../../_static/favicon.png" />
    <meta name="viewport" content="width=device-width,initial-scale=0.8">
    
    

  </head><body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="responsive-menu"><a href="#sidebar-anchor" title="Navigation">&#9776;</a></li>
        <li><a href="../../index.html">DistDL-0.5.0</a> &#187;</li>
          <li><a href="../index.html" >Code Reference</a> &#187;</li>
          <li><a href="../nn.html" accesskey="U">distdl.nn</a> &#187;</li> 
      </ul>
    </div>
    
        <div class="badge">
            <a href="https://github.com/microsoft/distdl/">Fork me on GitHub</a>
            <img src="../../_static/right-red@2x.png">
        </div>
    
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="reducescatter-layer">
<h1>ReduceScatter Layer<a class="headerlink" href="#reducescatter-layer" title="Permalink to this heading">¶</a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#overview" id="id1">Overview</a></p></li>
<li><p><a class="reference internal" href="#motivation" id="id2">Motivation</a></p></li>
<li><p><a class="reference internal" href="#implementation" id="id3">Implementation</a></p>
<ul>
<li><p><a class="reference internal" href="#assumptions" id="id4">Assumptions</a></p></li>
<li><p><a class="reference internal" href="#forward" id="id5">Forward</a></p></li>
<li><p><a class="reference internal" href="#adjoint" id="id6">Adjoint</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#reducescatter-rules" id="id7">ReduceScatter rules</a></p>
<ul>
<li><p><a class="reference internal" href="#standard-reducescatter" id="id8">Standard ReduceScatter</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#examples" id="id9">Examples</a></p></li>
<li><p><a class="reference internal" href="#api" id="id10">API</a></p></li>
</ul>
</nav>
<section id="overview">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Overview</a><a class="headerlink" href="#overview" title="Permalink to this heading">¶</a></h2>
<p>The ReduceScatter distributed data movement primitive reduces a distributed
tensor and then partitions it along a specified dimension or set of dimensions.</p>
<p>In DistDL, reduce-scatter collects data from (sub)tensors along slices of a
partition.  The reduce-scatter operation applies for partitions with and
without a (Cartesian) topology.</p>
<p>For the purposes of this documentation, we will assume that an arbitrary
global input tensor <span class="math notranslate nohighlight">\({x}\)</span> is partitioned by <span class="math notranslate nohighlight">\(P_x\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The definition of a reduce-scatter in DistDL goes beyond the classical parallel
collective operation, for example, <code class="docutils literal notranslate"><span class="pre">MPI_Reducescatter()</span></code> in MPI.  Such reductions
typically assume 1-dimensional arrays, scattered <em>within</em> a group of workers, and
neither impose nor exploit topological structure on the set of workers.</p>
</div>
</section>
<section id="motivation">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Motivation</a><a class="headerlink" href="#motivation" title="Permalink to this heading">¶</a></h2>
<p>In distributed deep learning, there are many applications of the reduce-scatter
primitive.  For example, linear layers in which weights are distributed along
the input channel dimension, use the reduce-scatter primitive after the matrix
multiplication to sum-reduce the output data and partition it along the same
dimension along which the input is distributed. In addition, reduce-scatter is
the adjoint of the all-gather primitive, and is therefore used in the backward
pass of distributed layers based on all-gather (such as layers using fully-sharded
data parallelism).</p>
</section>
<section id="implementation">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Implementation</a><a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h2>
<p>A back-end functional implementation supporting DistDL
<code class="xref py py-class docutils literal notranslate"><span class="pre">ReduceScatter</span></code> allows users to specify which dimensions
of the partition the gathering happens along.  No other options are
required because the all-gather occurs within the input partition.</p>
<section id="assumptions">
<h3><a class="toc-backref" href="#id4" role="doc-backlink">Assumptions</a><a class="headerlink" href="#assumptions" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>The reduce-scatter operation is <em>not</em> in-place.  Even if the operation is
equivalent to an identity (no dimensions are used in the reduction), a
Torch <code class="docutils literal notranslate"><span class="pre">.clone()</span></code> of the tensor is returned.</p></li>
<li><p>The current implementation only supports reduce-scattter along a <em>single</em>
partitioned dimension.</p></li>
</ul>
</section>
<section id="forward">
<h3><a class="toc-backref" href="#id5" role="doc-backlink">Forward</a><a class="headerlink" href="#forward" title="Permalink to this heading">¶</a></h3>
<p>The forward operation sums subtensors within <span class="math notranslate nohighlight">\(P_x\)</span> and then partitions
the output along a specified dimension.</p>
<ul class="simple">
<li><p>A worker that is active in <span class="math notranslate nohighlight">\(P_x\)</span> will take a subtensor of <span class="math notranslate nohighlight">\(x\)</span>
as input and return the sum of all subtensors as output, partitioned along
the specified dimension.</p></li>
<li><p>A worker that is not active in <span class="math notranslate nohighlight">\(P_x\)</span> will take a zero-volume tensor
as input and return a clone of that tensor as output.</p></li>
</ul>
<p>This class provides only an interface to the back-end implementation of the
forward algorithm.  This interface does not impose any mechanism for
performing the reduction and scattering. Performance details and optimizations
are back-end dependent.</p>
<p>The back-end forward operation is implemented through the <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html">PyTorch autograd</a> functional interface and
called through the ReduceScatter <span class="math notranslate nohighlight">\(~distdl.nn.ReduceScatter.forward\)</span> function.</p>
</section>
<section id="adjoint">
<h3><a class="toc-backref" href="#id6" role="doc-backlink">Adjoint</a><a class="headerlink" href="#adjoint" title="Permalink to this heading">¶</a></h3>
<p>The adjoint of the reduce-scatter primitive is the all-gather operation, which
collects subtensors within <span class="math notranslate nohighlight">\(P_x\)</span> and concatenates them along the given
data dimension.</p>
<p>This class provides only an interface to the back-end implementation of the
adjoint algorithm. This interface does not impose any mechanism for
performing this gathering. Performance details and optimizations are
back-end dependent.</p>
<p>The adjoint operation (PyTorch grad function class) is generated automatically
via autograd and calls the <code class="docutils literal notranslate"><span class="pre">backward()</span></code> function implemented by the back-end
functional interface.</p>
</section>
</section>
<section id="reducescatter-rules">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">ReduceScatter rules</a><a class="headerlink" href="#reducescatter-rules" title="Permalink to this heading">¶</a></h2>
<p>DistDL ReduceScatter layers will sum and partition the output along any cartesian dimension specified
by the user. This is different from the standard implementation of ReduceScatter in MPI and NCCL,
which views tensors as 1d arrays and partitions data along the last dimension.</p>
<p>ReduceScatter requires that subtensors have the same size across each rank, and that the tensor
dimension along which the operation is applied is evenly divisible by the partition size along
that dimension. For example, if the input tensor has shape <span class="math notranslate nohighlight">\([3, 7, 12]\)</span> and the partition
shape is <span class="math notranslate nohighlight">\([2, 2, 3]\)</span>, then the reduce-scatter operation can be applied along the last dimension,
with the output tensor having shape <span class="math notranslate nohighlight">\([3, 7, 4]\)</span>.</p>
<section id="standard-reducescatter">
<h3><a class="toc-backref" href="#id8" role="doc-backlink">Standard ReduceScatter</a><a class="headerlink" href="#standard-reducescatter" title="Permalink to this heading">¶</a></h3>
<section id="example-1">
<h4>Example 1<a class="headerlink" href="#example-1" title="Permalink to this heading">¶</a></h4>
<p>ReduceScatter operation on a 1D partition of shape <span class="math notranslate nohighlight">\(3\)</span>. The size of the input tensor must be
a multiple of the partition size, with a minimum tensor shape of 3.</p>
<figure class="align-default">
<img alt="Image of 1D reduce-scatter." src="../../_images/reduce_scatter_example_1d.png" />
</figure>
</section>
<section id="example-2">
<h4>Example 2<a class="headerlink" href="#example-2" title="Permalink to this heading">¶</a></h4>
<p>ReduceScatter operation on a 2D partition of shape <span class="math notranslate nohighlight">\(3 \times 2\)</span> along the first dimension. Data
is summed and partitioned along the first dimension. The size of the input tensor in the first dimension
must be a multiple of the partition size (i.e., <span class="math notranslate nohighlight">\(3\)</span>).</p>
<figure class="align-default">
<img alt="Image of 2D reduce-scatter." src="../../_images/reduce_scatter_example_2d.png" />
</figure>
</section>
<section id="example-3">
<h4>Example 3<a class="headerlink" href="#example-3" title="Permalink to this heading">¶</a></h4>
<p>ReduceScatter operation on a 3D partition of shape <span class="math notranslate nohighlight">\(3 \times 3 \times 3\)</span> along the last dimension.
Once again, the size of the input tensor in the last dimension must be a multiple of the partition size.</p>
<figure class="align-default">
<img alt="Image of 3D reduce-scatter." src="../../_images/reduce_scatter_example_3d.png" />
</figure>
</section>
</section>
</section>
<section id="examples">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Examples</a><a class="headerlink" href="#examples" title="Permalink to this heading">¶</a></h2>
<p>To reduce-scatter a 3-dimensional tensor that lives on a <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">x</span> <span class="pre">2</span> <span class="pre">x</span> <span class="pre">3</span></code> partition
along the last dimension:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">P_x_base</span> <span class="o">=</span> <span class="n">P_world</span><span class="o">.</span><span class="n">create_partition_inclusive</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">P_x</span> <span class="o">=</span> <span class="n">P_x_base</span><span class="o">.</span><span class="n">create_cartesian_topology_partition</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_local_shape</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">12</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">axes_reduce_scatter</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">ReduceScatter</span><span class="p">(</span><span class="n">P_x</span><span class="p">,</span> <span class="n">axes_reduce_scatter</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">zero_volume_tensor</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="n">P_x</span><span class="o">.</span><span class="n">active</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">x_local_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>The output tensor <span class="math notranslate nohighlight">\({y}\)</span> will have shape <code class="docutils literal notranslate"><span class="pre">[3,</span> <span class="pre">7,</span> <span class="pre">4]</span></code>.</p>
</section>
<section id="api">
<h2><a class="toc-backref" href="#id10" role="doc-backlink">API</a><a class="headerlink" href="#api" title="Permalink to this heading">¶</a></h2>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
    <a id="sidebar-anchor"></a>
    

<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script><h3><a href="../../index.html">Table Of Contents</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../user_guide/index.html">Using DistDL</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Code Reference</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../nn.html">distdl.nn</a></li>
<li class="toctree-l2"><a class="reference internal" href="../functional.html">distdl.functional</a></li>
<li class="toctree-l2"><a class="reference internal" href="../backends.html">distdl.backends</a></li>
<li class="toctree-l2"><a class="reference internal" href="../utilities.html">distdl.utilities</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../developer_guide/index.html">Contributor’s Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer_guide/index.html#contributing">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../release_history/index.html">Release History</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../acknowledgements.html">Citing &amp; Acknowledgements</a></li>
</ul>

  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../_sources/code_reference/nn/reduce_scatter.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
        </div>
<div id="sidebarbutton" title="Collapse sidebar">
<span>«</span>
</div>

      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="repartition.html" title="Repartition Layer"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="pooling.html" title="Pooling Layers"
             accesskey="P">previous</a> |</li>
      </ul>
    </div>

    <div class="footer" role="contentinfo">
        &#169; Copyright 2023, Microsoft.
      Last updated on May 02, 2023.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 6.2.1.
    </div>
  </body>
</html>