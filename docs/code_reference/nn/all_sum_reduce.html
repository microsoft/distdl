<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>AllSumReduce Layer &#8212; DistDL 0.5.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/pydoctheme.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script src="../../_static/sidebar.js"></script>
    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Broadcast Layer" href="broadcast.html" />
    <link rel="prev" title="AllGather Layer" href="all_gather.html" />
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <link rel="shortcut icon" type="image/png" href="../../_static/favicon.png" />
    <meta name="viewport" content="width=device-width,initial-scale=0.8">
    
    

  </head><body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="responsive-menu"><a href="#sidebar-anchor" title="Navigation">&#9776;</a></li>
        <li><a href="../../index.html">DistDL-0.5.0</a> &#187;</li>
          <li><a href="../index.html" >Code Reference</a> &#187;</li>
          <li><a href="../nn.html" accesskey="U">distdl.nn</a> &#187;</li> 
      </ul>
    </div>
    
        <div class="badge">
            <a href="https://github.com/microsoft/distdl/">Fork me on GitHub</a>
            <img src="../../_static/right-red@2x.png">
        </div>
    
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="allsumreduce-layer">
<h1>AllSumReduce Layer<a class="headerlink" href="#allsumreduce-layer" title="Permalink to this heading">¶</a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#overview" id="id1">Overview</a></p></li>
<li><p><a class="reference internal" href="#motivation" id="id2">Motivation</a></p></li>
<li><p><a class="reference internal" href="#implementation" id="id3">Implementation</a></p>
<ul>
<li><p><a class="reference internal" href="#assumptions" id="id4">Assumptions</a></p></li>
<li><p><a class="reference internal" href="#forward" id="id5">Forward</a></p></li>
<li><p><a class="reference internal" href="#adjoint" id="id6">Adjoint</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#examples" id="id7">Examples</a></p></li>
<li><p><a class="reference internal" href="#api" id="id8">API</a></p></li>
</ul>
</nav>
<section id="overview">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Overview</a><a class="headerlink" href="#overview" title="Permalink to this heading">¶</a></h2>
<p>The AllSumReduce distributed data movement primitive sums data on a within a
set of workers in a Partition.</p>
<p>In DistDL, all-sum-reductions sum data from (sub)tensors along slices of a
partition.  The all-sum-reduce operation applies for partitions with and
without a (Cartesian) topology.</p>
<p>For the purposes of this documentation, we will assume that an arbitrary
global input tensor <span class="math notranslate nohighlight">\({x}\)</span> is partitioned by <span class="math notranslate nohighlight">\(P_x\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The definition of a all-sum-reduction in DistDL goes beyond the classical
parallel reduction operation, for example, <code class="docutils literal notranslate"><span class="pre">MPI_Allreduce()</span></code> in MPI.  Such
reductions typically assume 1-dimensional arrays, reduced <em>within</em> a group
of workers, and neither impose nor exploit topological structure on the set
of workers.</p>
</div>
</section>
<section id="motivation">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Motivation</a><a class="headerlink" href="#motivation" title="Permalink to this heading">¶</a></h2>
<p>In distributed deep learning, there are many applications of the all-reduction
primitive.  For example, in normalization layers, including
distributed <span class="xref std std-ref">code_reference/nn/batchnorm:Batch Normalization Layers</span>,
global tensor statistics are required on all workers.</p>
</section>
<section id="implementation">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Implementation</a><a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h2>
<p>A back-end functional implementation supporting DistDL
<code class="xref py py-class docutils literal notranslate"><span class="pre">AllSumReduce</span></code> allows users to specify which dimensions
of the partition the reductions happen along.  No other options are
required because the all-reduction occurs within the input partition.</p>
<p>Input tensors may be partitioned by this partition but they are not required
to be.  If the AllSumReduce is part of a broader all-sum-reduction on a
tensor (along specific dimesions) then the local reduction must be performed
first and the distributed reduction afterward.  (This is normal for
distributed all-reductions.)</p>
<section id="assumptions">
<h3><a class="toc-backref" href="#id4" role="doc-backlink">Assumptions</a><a class="headerlink" href="#assumptions" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>The all-sum-reduction operation is <em>not</em> in-place.  Even if the operation is
equivalent to an identity (no dimensions are used in the reduction), a
Torch <code class="docutils literal notranslate"><span class="pre">.clone()</span></code> of the tensor is returned.</p></li>
</ul>
</section>
<section id="forward">
<h3><a class="toc-backref" href="#id5" role="doc-backlink">Forward</a><a class="headerlink" href="#forward" title="Permalink to this heading">¶</a></h3>
<p>The forward operation sums subtensors within <span class="math notranslate nohighlight">\(P_x\)</span>, within subpartitions
of the input partition, as specified by the user, and broadcasts
the results within the same subpartition.  The reduce and broadcast operations
are not necessarily explicit.</p>
<ul class="simple">
<li><p>A worker that is active in <span class="math notranslate nohighlight">\(P_x\)</span> will take a subtensor
of <span class="math notranslate nohighlight">\(x\)</span> as input and return a subtensor of <span class="math notranslate nohighlight">\(y\)</span> as output.</p></li>
<li><p>A worker that is not active in <span class="math notranslate nohighlight">\(P_x\)</span> will take a zero-volume tensor
as input and return a clone of that tensor as output.</p></li>
</ul>
<p>This class provides only an interface to the back-end implementation of the
forward algorithm.  This interface does not impose any mechanism for
performing the reduction.  Performance details and optimizations are back-end
dependent.</p>
<p>The back-end forward operation is implemented through the <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html">PyTorch autograd</a> functional interface and
called through the AllSumReduce <span class="math notranslate nohighlight">\(~distdl.nn.AllSumReduce.forward\)</span> function.</p>
</section>
<section id="adjoint">
<h3><a class="toc-backref" href="#id6" role="doc-backlink">Adjoint</a><a class="headerlink" href="#adjoint" title="Permalink to this heading">¶</a></h3>
<p>AllSumReduce is self-adjoint.  Thus, the adjoint (backward) operation is exactly
the same as the forward operation.</p>
<p>This class provides only an interface to the back-end implementation of the
adjoint algorithm.  This interface does not impose any mechanism for
performing this broadcast.  Performance details and optimizations are
back-end dependent.</p>
<p>The adjoint operation (PyTorch grad function class) is generated automatically
via autograd and calls the <code class="docutils literal notranslate"><span class="pre">backward()</span></code> function implemented by the back-end
functional interface.</p>
</section>
</section>
<section id="examples">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">Examples</a><a class="headerlink" href="#examples" title="Permalink to this heading">¶</a></h2>
<p>To reduce a 3-dimensional tensor that lives on a <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">x</span> <span class="pre">2</span> <span class="pre">x</span> <span class="pre">3</span></code> partition
along the last two dimesions:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">P_x_base</span> <span class="o">=</span> <span class="n">P_world</span><span class="o">.</span><span class="n">create_partition_inclusive</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">P_x</span> <span class="o">=</span> <span class="n">P_x_base</span><span class="o">.</span><span class="n">create_cartesian_topology_partition</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_local_shape</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">axes_reduce</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">AllSumReduce</span><span class="p">(</span><span class="n">P_x</span><span class="p">,</span> <span class="n">axes_reduce</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">zero_volume_tensor</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="n">P_x</span><span class="o">.</span><span class="n">active</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">x_local_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>Here, each subtensor of <span class="math notranslate nohighlight">\({y}\)</span> is the sum of the subtensors of
<span class="math notranslate nohighlight">\({x}\)</span> from 6 workers.</p>
</section>
<section id="api">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">API</a><a class="headerlink" href="#api" title="Permalink to this heading">¶</a></h2>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
    <a id="sidebar-anchor"></a>
    

<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script><h3><a href="../../index.html">Table Of Contents</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../user_guide/index.html">Using DistDL</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Code Reference</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../nn.html">distdl.nn</a></li>
<li class="toctree-l2"><a class="reference internal" href="../functional.html">distdl.functional</a></li>
<li class="toctree-l2"><a class="reference internal" href="../backends.html">distdl.backends</a></li>
<li class="toctree-l2"><a class="reference internal" href="../utilities.html">distdl.utilities</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../developer_guide/index.html">Contributor’s Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer_guide/index.html#contributing">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../release_history/index.html">Release History</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../acknowledgements.html">Citing &amp; Acknowledgements</a></li>
</ul>

  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../_sources/code_reference/nn/all_sum_reduce.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
        </div>
<div id="sidebarbutton" title="Collapse sidebar">
<span>«</span>
</div>

      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="broadcast.html" title="Broadcast Layer"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="all_gather.html" title="AllGather Layer"
             accesskey="P">previous</a> |</li>
      </ul>
    </div>

    <div class="footer" role="contentinfo">
        &#169; Copyright 2023, Microsoft.
      Last updated on May 02, 2023.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 6.2.1.
    </div>
  </body>
</html>