<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Using DistDL &#8212; DistDL 0.5.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/pydoctheme.css" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css" />
    
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script src="../_static/sidebar.js"></script>
    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Code Reference" href="../code_reference/index.html" />
    <link rel="prev" title="Contents" href="../index.html" />
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <link rel="shortcut icon" type="image/png" href="../_static/favicon.png" />
    <meta name="viewport" content="width=device-width,initial-scale=0.8">
    
    

  </head><body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="responsive-menu"><a href="#sidebar-anchor" title="Navigation">&#9776;</a></li>
        <li><a href="../index.html">DistDL-0.5.0</a> &#187;</li> 
      </ul>
    </div>
    
        <div class="badge">
            <a href="https://github.com/microsoft/distdl/">Fork me on GitHub</a>
            <img src="../_static/right-red@2x.png">
        </div>
    
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="using-distdl">
<h1>Using DistDL<a class="headerlink" href="#using-distdl" title="Permalink to this heading">¶</a></h1>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this heading">¶</a></h2>
<p>At the command line:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">git</span> <span class="n">clone</span> <span class="n">git</span><span class="nd">@github</span><span class="o">.</span><span class="n">com</span><span class="p">:</span><span class="n">microsoft</span><span class="o">/</span><span class="n">distdl</span><span class="o">.</span><span class="n">git</span>
<span class="n">cd</span> <span class="n">distdl</span>
<span class="n">pip3</span> <span class="n">install</span> <span class="o">-</span><span class="n">e</span> <span class="o">.</span>
</pre></div>
</div>
</section>
<section id="usage">
<h2>Usage<a class="headerlink" href="#usage" title="Permalink to this heading">¶</a></h2>
<p>To use DistDL in a project:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">distdl</span>
</pre></div>
</div>
</section>
<section id="design-philosophy">
<h2>Design Philosophy<a class="headerlink" href="#design-philosophy" title="Permalink to this heading">¶</a></h2>
<p>DistDL provides a “model parallelism” for deep learning, built on the
<span class="xref std std-ref">PyTorch</span> library.</p>
<p>Most parallelism in deep learning toolkits arises from “data parallelism,”
which essentially relies on the fact that different input data, often grouped
into batches, have independent impact on the trained network and can thus be
assimilated in embarrassingly parallel fashion.</p>
<p>Model parallelism has been more challenging to achieve, partially because it
is not as well-defined in the context of deep neural networks.  Traditionally,
e.g., in the context of PDE solvers, the model to be parallelized comes from a
spatial decomposition of the physical properties driving the simulation.  In a
general neural network, such an abstraction does not exist.  Instead, we have
tensors, many of which only have tenuous connection to reality and often have
no auxiliary structure over which we can decompose a problem.</p>
<p>As a result, DistDL approaches the distributed deep learning problem by
assuming the optimal decomposition for <em>any and all tensors in the network</em>
should drive the decomposition.  Here, the definition of <em>optimal</em> is entirely
problem, computing system, and network architecture dependent.</p>
<p>We provide a general framework from which optimal implementations of our data
movement primitives for distributed tensors can be implemented.  This
framework is designed to allow fine tuning of back-ends that are optimal for a
variety of parallel computer architectures, communication back-ends, and
optimality criteria.</p>
<p>To reflect this generality, we attempt to eschew some back-end specific terminology,
and generally assume that parallel computation is performed by a number of
<strong>workers</strong> that communicate with each other using <strong>data movement</strong> primitives.
We will refer to a collection of workers as a <strong>team</strong> of workers.</p>
<section id="tensors">
<h3>Tensors<a class="headerlink" href="#tensors" title="Permalink to this heading">¶</a></h3>
<p>DistDL assumes that tensors representing inputs, outputs, and network parameters
are PyTorch <a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><code class="xref any docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> objects.</p>
<section id="zero-volume-tensors">
<h4>Zero-volume Tensors<a class="headerlink" href="#zero-volume-tensors" title="Permalink to this heading">¶</a></h4>
<p>Occasionally, there are more workers available than are required to store and
compute over a distributed tensor.  Some distributed layers may require more
workers to store their outputs than are required to store the inputs, or vice
versa.  Also, some layers (for example <span class="xref std std-ref">distributed linear layers</span>) may require more workers to perform
computation than are required to store the inputs and outputs.  In these cases
and because all workers in a given team may be needed, DistDL occasionally
expects tensors with zero-volume as input or output.</p>
<figure class="align-default" id="id1">
<img alt="An example distributed layer with zero-volume inputs and outputs." src="../_images/zero_volume_tensor.png" />
<figcaption>
<p><span class="caption-text">(left) A sequential layer, with size 10 input and sized 6 output.
(right) A distributed layer, performing the same operation, but with an input
partition using the first 3 of the 4 workers and an output partition using
the last 2 of the 4 workers.  <span class="math notranslate nohighlight">\(\varnothing\)</span> indicates a zero-volume input
or output.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>In PyTorch, a tensor with zero-volume is one that has a shape containing a zero.
A convenience function, <code class="xref any docutils literal notranslate"><span class="pre">distdl.utilities.torch.zero_volume_tensor</span></code>, is
provided to aid in the creation of these tensors.</p>
</section>
</section>
<section id="tensor-partitions">
<h3>Tensor Partitions<a class="headerlink" href="#tensor-partitions" title="Permalink to this heading">¶</a></h3>
<p>At the core of DistDL is the concept of the <strong>tensor partition</strong>.  In DistDL,
any tensor may be a distributed tensor.  Distributed tensors generally fall
into two categories:</p>
<ol class="arabic simple">
<li><p>tensors distributed over workers with no implicit or explicit ordering
induced by a topology, and</p></li>
<li><p>tensors distributed over workers that have an implicit or explicit ordering
induced by a Cartesian topology.</p></li>
</ol>
<p>Tensors in the former category often appear when a copy of the single tensor
is needed by multiple workers.  For example the weight tensor for a simple
convolutional layer will be needed on multiple workers, if each worker is
responsible for applying the kernel to a portion of the input tensor.  Tensor
partitions over tensors in this class are created using the back-end
implementation of the <code class="xref any docutils literal notranslate"><span class="pre">Partition</span></code> class.</p>
<figure class="align-default" id="id2">
<img alt="A tensor partition of size 4 with no topology." src="../_images/tensor_partition_no_topology.png" />
<figcaption>
<p><span class="caption-text">A <span class="math notranslate nohighlight">\(4 \times 4 \times 3\)</span> tensor replicated onto a partition of size 4
with no topology.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When tensors are distributed over a partition using replication in a forward
pass, the gradient tensors that appear on the partition are not necessarily
copies of each other.</p>
</div>
<p>Tensors in the latter category generally appear as input and outputs of
distributed layers, where each worker is responsible for processing only a
portion of the input and producing a portion of the output.  Tensor partitions
over tensors in this class are created using the back-end implementation of
the <code class="xref any docutils literal notranslate"><span class="pre">CartesianPartition</span></code> class.</p>
<figure class="align-default" id="id3">
<img alt="A tensor partition of size 4 with 1x2x2 Cartesian topology." src="../_images/tensor_partition_cart_topology.png" />
<figcaption>
<p><span class="caption-text">A <span class="math notranslate nohighlight">\(4 \times 4 \times 3\)</span> tensor partitioned onto a partition of size 4
with a <span class="math notranslate nohighlight">\(1 \times 2 \times 2\)</span> Cartesian topology.</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The technical structure of each of these partitions is back-end specific. See
the <a class="reference internal" href="../code_reference/index.html#code-reference"><span class="std std-ref">Code Reference</span></a> for the
back-end <a class="reference internal" href="../code_reference/backends.html#backends"><span class="std std-ref">back-end</span></a> for implementation
details.</p>
</section>
<section id="distributed-layers">
<h3>Distributed Layers<a class="headerlink" href="#distributed-layers" title="Permalink to this heading">¶</a></h3>
<p>DistDL defines a number of distributed layer functions, implemented as
PyTorch modules, which allow distributed neural networks to be constructed
in the same way PyTorch allows sequential networks to be constructed.</p>
<p>DistDL distributed layers are implemented following the linear algebraic model
provided in the paper <a class="reference external" href="https://arxiv.org/abs/2006.03108">*A Linear Algebraic Approach to Model Parallelism in
Deep Learning*</a>.  Specific details of the
interface are documented in the <a class="reference internal" href="../code_reference/index.html#code-reference"><span class="std std-ref">Code Reference</span></a> and back-end specific details are
documented with that <a class="reference internal" href="../code_reference/backends.html#backends"><span class="std std-ref">back-end</span></a>.</p>
<p>All layers are assumed to be inductively load-balanced.  That is, their output
should be load balanced so that each worker does approximately equal work.
Consequently, it is assumed that the inputs are also load balanced, as a load
balanced output from one layer is the input to the next.</p>
</section>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
    <a id="sidebar-anchor"></a>
    

<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script><h3><a href="../index.html">Table Of Contents</a></h3>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Using DistDL</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#installation">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#usage">Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="#design-philosophy">Design Philosophy</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../code_reference/index.html">Code Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/index.html">Contributor’s Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/index.html#contributing">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release_history/index.html">Release History</a></li>
<li class="toctree-l1"><a class="reference internal" href="../acknowledgements.html">Citing &amp; Acknowledgements</a></li>
</ul>

  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/user_guide/index.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
        </div>
<div id="sidebarbutton" title="Collapse sidebar">
<span>«</span>
</div>

      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../code_reference/index.html" title="Code Reference"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../index.html" title="Contents"
             accesskey="P">previous</a> |</li>
      </ul>
    </div>

    <div class="footer" role="contentinfo">
        &#169; Copyright 2023, Microsoft.
      Last updated on May 02, 2023.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 6.2.1.
    </div>
  </body>
</html>