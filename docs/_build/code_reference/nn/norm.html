<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Normalization Layers &#8212; DistDL 0.5.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/pydoctheme.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script src="../../_static/sidebar.js"></script>
    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Pooling Layers" href="pooling.html" />
    <link rel="prev" title="Base Distributed Module" href="module.html" />
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <link rel="shortcut icon" type="image/png" href="../../_static/favicon.png" />
    <meta name="viewport" content="width=device-width,initial-scale=0.8">
    
    

  </head><body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="responsive-menu"><a href="#sidebar-anchor" title="Navigation">&#9776;</a></li>
        <li><a href="../../index.html">DistDL-0.5.0</a> &#187;</li>
          <li><a href="../index.html" >Code Reference</a> &#187;</li>
          <li><a href="../nn.html" accesskey="U">distdl.nn</a> &#187;</li> 
      </ul>
    </div>
    
        <div class="badge">
            <a href="https://github.com/microsoft/distdl/">Fork me on GitHub</a>
            <img src="../../_static/right-red@2x.png">
        </div>
    
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="normalization-layers">
<h1>Normalization Layers<a class="headerlink" href="#normalization-layers" title="Permalink to this heading">¶</a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#batch-normalization" id="id5">Batch Normalization</a></p>
<ul>
<li><p><a class="reference internal" href="#overview" id="id6">Overview</a></p></li>
<li><p><a class="reference internal" href="#implementation" id="id7">Implementation</a></p></li>
<li><p><a class="reference internal" href="#example" id="id8">Example</a></p></li>
<li><p><a class="reference internal" href="#api" id="id9">API</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#layer-normalization" id="id10">Layer Normalization</a></p>
<ul>
<li><p><a class="reference internal" href="#id1" id="id11">Overview</a></p></li>
<li><p><a class="reference internal" href="#id2" id="id12">Implementation</a></p></li>
<li><p><a class="reference internal" href="#id3" id="id13">Example</a></p></li>
<li><p><a class="reference internal" href="#id4" id="id14">API</a></p></li>
</ul>
</li>
</ul>
</nav>
<section id="batch-normalization">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Batch Normalization</a><a class="headerlink" href="#batch-normalization" title="Permalink to this heading">¶</a></h2>
<section id="overview">
<h3><a class="toc-backref" href="#id6" role="doc-backlink">Overview</a><a class="headerlink" href="#overview" title="Permalink to this heading">¶</a></h3>
<p>DistDL supports distributed batch normalization through the <code class="docutils literal notranslate"><span class="pre">distdl.nn.DistributedBatchNorm</span></code> layer.
Unlike PyTorch, DistDL does not provide separate modules for 1D, 2D, and 3D batch normalization, but
instead the dimension is inferred automatically from the input partition <span class="math notranslate nohighlight">\(P_x\)</span>.</p>
<p>Communication of the batch normalization layer depends of the input partitioning scheme. Tensors
that are partitioned along the channel-dimension only, do not induce communication, as batch
normalization does not average over the channel-dimension. Tensors that are partitioned over any other
dimensions are averaged locally first, and then a sum-reduction over the partitioned dimensions is
performed.</p>
<figure class="align-default">
<img alt="Example for distributed batch normalization." src="../../_images/batch_normalization_example.png" />
</figure>
</section>
<section id="implementation">
<h3><a class="toc-backref" href="#id7" role="doc-backlink">Implementation</a><a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h3>
<p>DistDL’s <code class="docutils literal notranslate"><span class="pre">nn.DistributedBatchNorm</span></code> layer is a generalization of PyTorch’s <code class="docutils literal notranslate"><span class="pre">nn.BatchNormNd</span></code> layer
that supports arbitrary input partitioning schemes. If learnable affine parameters are used, weights
are broadcasted to all data-parallel workers during the forward pass. Mean and variances are computed
locally, and then (sum-)reduced over the partitioned dimensions.</p>
</section>
<section id="example">
<h3><a class="toc-backref" href="#id8" role="doc-backlink">Example</a><a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">P_x_base</span> <span class="o">=</span> <span class="n">P_world</span><span class="o">.</span><span class="n">create_partition_inclusive</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">P_x</span> <span class="o">=</span> <span class="n">P_x_base</span><span class="o">.</span><span class="n">create_cartesian_topology_partition</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">6</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">num_tokens</span> <span class="o">=</span> <span class="mi">7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">num_features</span> <span class="o">=</span> <span class="mi">32</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_local_shape</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">DistributedBatchNorm</span><span class="p">(</span><span class="n">P_x</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">zero_volume_tensor</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="n">P_x</span><span class="o">.</span><span class="n">active</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">x_local_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="api">
<h3><a class="toc-backref" href="#id9" role="doc-backlink">API</a><a class="headerlink" href="#api" title="Permalink to this heading">¶</a></h3>
</section>
</section>
<section id="layer-normalization">
<h2><a class="toc-backref" href="#id10" role="doc-backlink">Layer Normalization</a><a class="headerlink" href="#layer-normalization" title="Permalink to this heading">¶</a></h2>
<section id="id1">
<h3><a class="toc-backref" href="#id11" role="doc-backlink">Overview</a><a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h3>
<p>DistDL’s distributed layer normalization is a generalization of PyTorch’s  <code class="docutils literal notranslate"><span class="pre">nn.LayerNorm</span></code> module.
The layer follows the same conventions as PyTorch’s version. Namely, mean and variance are computed
over the last <span class="math notranslate nohighlight">\(D\)</span> dimensions of the input partitioning scheme, where <span class="math notranslate nohighlight">\(D\)</span> is the number of
dimensions of <cite>normalized_shape</cite>.</p>
<p>The communication patterns of the layer normalization module are induced by the partitioning scheme of
the input tensor. As for batch normalization, input partitions with any number of dimensions and sizes
in each dimension are supported. Tensors that are partitioned along the batch dimension only (i.e. data
parallelism), do not induce communication for the mean and variances. Other partitioning schemes
require a local reduction of the mean and variances, followed by a sum-reduction.</p>
<figure class="align-default">
<img alt="Example for distributed layer normalization." src="../../_images/layer_normalization_example.png" />
</figure>
</section>
<section id="id2">
<h3><a class="toc-backref" href="#id12" role="doc-backlink">Implementation</a><a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h3>
<p>The distributed layer normalization module is implemented as <code class="docutils literal notranslate"><span class="pre">distdl.nn.DistributedLayerNorm</span></code>. The
module has the same function signature as PyTorch’s version with the addition of the partitioning
scheme of the input tensor. The module supports learnable affine parameters, which are broadcasted to
all data-parallel workers during the forward pass. Mean and variances are computed locally, and then
(sum-)reduced over the partitioned dimensions.</p>
</section>
<section id="id3">
<h3><a class="toc-backref" href="#id13" role="doc-backlink">Example</a><a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">P_x_base</span> <span class="o">=</span> <span class="n">P_world</span><span class="o">.</span><span class="n">create_partition_inclusive</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">P_x</span> <span class="o">=</span> <span class="n">P_x_base</span><span class="o">.</span><span class="n">create_cartesian_topology_partition</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">6</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">num_tokens</span> <span class="o">=</span> <span class="mi">7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">num_features</span> <span class="o">=</span> <span class="mi">32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">normalized_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_local_shape</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">DistributedLayerNorm</span><span class="p">(</span><span class="n">P_x</span><span class="p">,</span> <span class="n">normalized_shape</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">zero_volume_tensor</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="n">P_x</span><span class="o">.</span><span class="n">active</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">x_local_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id4">
<h3><a class="toc-backref" href="#id14" role="doc-backlink">API</a><a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h3>
</section>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
    <a id="sidebar-anchor"></a>
    

<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script><h3><a href="../../index.html">Table Of Contents</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../user_guide/index.html">Using DistDL</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Code Reference</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../nn.html">distdl.nn</a></li>
<li class="toctree-l2"><a class="reference internal" href="../functional.html">distdl.functional</a></li>
<li class="toctree-l2"><a class="reference internal" href="../backends.html">distdl.backends</a></li>
<li class="toctree-l2"><a class="reference internal" href="../utilities.html">distdl.utilities</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../developer_guide/index.html">Contributor’s Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer_guide/index.html#contributing">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../release_history/index.html">Release History</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../acknowledgements.html">Citing &amp; Acknowledgements</a></li>
</ul>

  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../_sources/code_reference/nn/norm.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
        </div>
<div id="sidebarbutton" title="Collapse sidebar">
<span>«</span>
</div>

      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="pooling.html" title="Pooling Layers"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="module.html" title="Base Distributed Module"
             accesskey="P">previous</a> |</li>
      </ul>
    </div>

    <div class="footer" role="contentinfo">
        &#169; Copyright 2023, Microsoft.
      Last updated on May 02, 2023.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 6.2.1.
    </div>
  </body>
</html>