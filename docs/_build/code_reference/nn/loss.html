<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Loss Functions &#8212; DistDL 0.5.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/pydoctheme.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script src="../../_static/sidebar.js"></script>
    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Base Distributed Module" href="module.html" />
    <link rel="prev" title="Linear Layer" href="linear.html" />
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <link rel="shortcut icon" type="image/png" href="../../_static/favicon.png" />
    <meta name="viewport" content="width=device-width,initial-scale=0.8">
    
    

  </head><body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="responsive-menu"><a href="#sidebar-anchor" title="Navigation">&#9776;</a></li>
        <li><a href="../../index.html">DistDL-0.5.0</a> &#187;</li>
          <li><a href="../index.html" >Code Reference</a> &#187;</li>
          <li><a href="../nn.html" accesskey="U">distdl.nn</a> &#187;</li> 
      </ul>
    </div>
    
        <div class="badge">
            <a href="https://github.com/microsoft/distdl/">Fork me on GitHub</a>
            <img src="../../_static/right-red@2x.png">
        </div>
    
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="loss-functions">
<h1>Loss Functions<a class="headerlink" href="#loss-functions" title="Permalink to this heading">¶</a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#overview" id="id1">Overview</a></p></li>
<li><p><a class="reference internal" href="#implementation" id="id2">Implementation</a></p>
<ul>
<li><p><a class="reference internal" href="#assumptions" id="id3">Assumptions</a></p></li>
<li><p><a class="reference internal" href="#forward" id="id4">Forward</a></p></li>
<li><p><a class="reference internal" href="#adjoint" id="id5">Adjoint</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#examples" id="id6">Examples</a></p></li>
<li><p><a class="reference internal" href="#api" id="id7">API</a></p></li>
</ul>
</nav>
<section id="overview">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Overview</a><a class="headerlink" href="#overview" title="Permalink to this heading">¶</a></h2>
<p>DistDL provides distributed implementations of many PyTorch loss functions.</p>
<p>For the purposes of this documentation, we will assume that arbitrary global
input and target tensors <span class="math notranslate nohighlight">\({x}\)</span> and <span class="math notranslate nohighlight">\({y}\)</span> are partitioned
by <span class="math notranslate nohighlight">\(P_x\)</span>.</p>
</section>
<section id="implementation">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Implementation</a><a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h2>
<p>DistDL distributed loss functions are essentially wrappers around their
corresponding PyTorch loss functions, with the reductions computed properly
for parallel environments.  When reduced, the loss value is on a single worker.</p>
<p>For <cite>reduction=”none”</cite>, as with the PyTorch losses, no reduction is performed
and each worker has its component of the element-wise loss.</p>
<p>When <cite>reduction</cite> is <code class="docutils literal notranslate"><span class="pre">&quot;sum&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code> or (<code class="docutils literal notranslate"><span class="pre">&quot;batchmean&quot;</span></code>), each worker
computes a local sum (the <cite>reduction</cite> mode for the base PyTorch layer is
<code class="docutils literal notranslate"><span class="pre">&quot;sum&quot;</span></code>) and the appropriate normalization factor (<span class="math notranslate nohighlight">\(1\)</span>, the total
features, or the batch size) is applied <em>after</em> a DistDL SumReduce layer is
used to reduce the loss to the root worker.</p>
<p>The root partition is assembled when the distributed loss is instantiated and
consists, always, of the <span class="math notranslate nohighlight">\(0^{\text{th}}\)</span> worker.  After the call,
the <span class="math notranslate nohighlight">\(0^{\text{th}}\)</span> worker in <span class="math notranslate nohighlight">\(P_x\)</span> has the true loss and all
other workers have invalid values.</p>
<p>PyTorch requires loss functions to be scalars (wrapped in a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>) for the
<code class="docutils literal notranslate"><span class="pre">backward()</span></code> method to work.  In
DistDL, <a class="reference internal" href="sum_reduce.html#sumreduce-layer"><span class="std std-ref">SumReduce Layer</span></a> layers return
zero-volume tensors for workers that are not in the output partition. To
prevent optimization loops from needing to branch on the <span class="math notranslate nohighlight">\(0^{\text
{th}}\)</span> worker to call <code class="docutils literal notranslate"><span class="pre">backward()</span></code>, distributed losses use the
<code class="docutils literal notranslate"><span class="pre">ZeroVolumeCorrectorFunction()</span></code> to convert zero-volume outputs to
meaningless scalar tensors in a <code class="docutils literal notranslate"><span class="pre">forward()</span></code> call and to convert any grad
input back to zero-volume tensors during the <code class="docutils literal notranslate"><span class="pre">backward()</span></code> phase.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>DistDL distributed loss functions follow DistDL’s design principles: the
communication is part of the mathematical formulation of the distributed
network.  Thus, we do not all-reduce the result.  Only one worker has the
true loss.</p>
<p>However, our approach is <em>equivalent</em> to those that do perform the
all-reduce. If an all-reduce is applied and the result is normalized,
technically, nothing needs to be done in the adjoint phase.  The adjoint
would be another normalized all-reduction, which is essentially the
identity.</p>
<p>Here, the forward operation includes only a sum-reduction, which induces
a broadcast in the adjoint operation.  This sum-reduction followed by a
broadcast is <em>precisely</em> an all-reduction.  However, it is induced
naturally rather than imposed externally.</p>
</div>
<section id="assumptions">
<h3><a class="toc-backref" href="#id3" role="doc-backlink">Assumptions</a><a class="headerlink" href="#assumptions" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>The global input tensor <span class="math notranslate nohighlight">\(x\)</span> has shape <span class="math notranslate nohighlight">\(n_{\text{batch}} \times
n_{D-1} \times \cdots \times n_0\)</span>, where <span class="math notranslate nohighlight">\(D\)</span> is number of channel
and feature dimensions.</p></li>
<li><p>The global target tensor <span class="math notranslate nohighlight">\(y\)</span> has the same shape as <span class="math notranslate nohighlight">\(x\)</span> and is
distributed such that the local shapes also match.</p></li>
<li><p>The input partition <span class="math notranslate nohighlight">\(P_x\)</span> has shape <span class="math notranslate nohighlight">\(P_{\text{b}} \times P_{D-1}
\times \cdots \times P_0\)</span>, where <span class="math notranslate nohighlight">\(P_{d}\)</span> is the number of workers
partitioning the <span class="math notranslate nohighlight">\(d^{\text{th}}\)</span> channel or feature dimension of
<span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
<li><p>The worker with rank 0 returns the global loss, which has the same value as
if it were computed sequentially.  All other workers return a scalar with
value <span class="math notranslate nohighlight">\(0.0\)</span>.</p></li>
</ul>
</section>
<section id="forward">
<h3><a class="toc-backref" href="#id4" role="doc-backlink">Forward</a><a class="headerlink" href="#forward" title="Permalink to this heading">¶</a></h3>
<p>Under the above assumptions, the forward algorithm is:</p>
<ol class="arabic simple">
<li><p>Compute the local loss.  If the reduction mode is <code class="docutils literal notranslate"><span class="pre">&quot;none&quot;</span></code>, return the
result of the PyTorch layer on the local input and target.  If another
reduction is specified, apply the <code class="docutils literal notranslate"><span class="pre">&quot;sum&quot;</span></code> reduction mode to the local
layer.</p></li>
<li><p>Use a <a class="reference internal" href="sum_reduce.html#sumreduce-layer"><span class="std std-ref">SumReduce Layer</span></a> to reduce
the local losses to the root worker.</p></li>
<li><p>On the <span class="math notranslate nohighlight">\(0^{\text{th}}\)</span> worker, apply the correct normalization based
on the <cite>reduction</cite> mode.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The normalization constant is computed in a pre-forward hook so that
it can be re-used without more collective communication.</p>
</div>
<ol class="arabic simple" start="4">
<li><p>For the <span class="math notranslate nohighlight">\(0^{\text{th}}\)</span> worker, return the global loss.  For all
other workers in <span class="math notranslate nohighlight">\(P_x\)</span>, return a scalar with value <span class="math notranslate nohighlight">\(0.0\)</span>.</p></li>
</ol>
</section>
<section id="adjoint">
<h3><a class="toc-backref" href="#id5" role="doc-backlink">Adjoint</a><a class="headerlink" href="#adjoint" title="Permalink to this heading">¶</a></h3>
<p>The adjoint algorithm is not explicitly implemented.  PyTorch’s <code class="docutils literal notranslate"><span class="pre">autograd</span></code>
feature automatically builds the adjoint of the Jacobian of the distributed
loss calculation.  Essentially, the algorithm is as follows:</p>
<ol class="arabic simple">
<li><p>For the <span class="math notranslate nohighlight">\(0^{\text{th}}\)</span> worker, the gradient output (input to
<code class="docutils literal notranslate"><span class="pre">backward()</span></code>) is preserved. For all other workers, convert that input to a
zero-volume tensor.</p></li>
<li><p>On the <span class="math notranslate nohighlight">\(0^{\text{th}}\)</span> worker, apply the adjoint of the normalization.</p></li>
<li><p>Broadcast the gradient output to all workers in <span class="math notranslate nohighlight">\(P_x\)</span>.  This is the
adjoint of the forward sum-reduce.</p></li>
<li><p>Compute the local adjoint application.</p></li>
</ol>
</section>
</section>
<section id="examples">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Examples</a><a class="headerlink" href="#examples" title="Permalink to this heading">¶</a></h2>
<p>To apply a distributed loss layer on tensors mapped to a <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">x</span> <span class="pre">4</span></code> partition:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">P_x_base</span> <span class="o">=</span> <span class="n">P_world</span><span class="o">.</span><span class="n">create_partition_inclusive</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">P_x</span> <span class="o">=</span> <span class="n">P_x_base</span><span class="o">.</span><span class="n">create_cartesian_topology_partition</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_local_shape</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">40</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">criterion</span> <span class="o">=</span> <span class="n">DistributedMSELoss</span><span class="p">(</span><span class="n">P_x</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">zero_volume_tensor</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">zero_volume_tensor</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="n">P_x</span><span class="o">.</span><span class="n">active</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">x_local_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">x_local_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="api">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">API</a><a class="headerlink" href="#api" title="Permalink to this heading">¶</a></h2>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
    <a id="sidebar-anchor"></a>
    

<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script><h3><a href="../../index.html">Table Of Contents</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../user_guide/index.html">Using DistDL</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Code Reference</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../nn.html">distdl.nn</a></li>
<li class="toctree-l2"><a class="reference internal" href="../functional.html">distdl.functional</a></li>
<li class="toctree-l2"><a class="reference internal" href="../backends.html">distdl.backends</a></li>
<li class="toctree-l2"><a class="reference internal" href="../utilities.html">distdl.utilities</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../developer_guide/index.html">Contributor’s Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer_guide/index.html#contributing">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../release_history/index.html">Release History</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../acknowledgements.html">Citing &amp; Acknowledgements</a></li>
</ul>

  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../_sources/code_reference/nn/loss.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
        </div>
<div id="sidebarbutton" title="Collapse sidebar">
<span>«</span>
</div>

      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="module.html" title="Base Distributed Module"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="linear.html" title="Linear Layer"
             accesskey="P">previous</a> |</li>
      </ul>
    </div>

    <div class="footer" role="contentinfo">
        &#169; Copyright 2023, Microsoft.
      Last updated on May 02, 2023.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 6.2.1.
    </div>
  </body>
</html>